{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jupyter notebooks\n",
    "\n",
    "This is a [Jupyter](http://jupyter.org/) notebook using Python.  You can install Jupyter locally to edit and interact with this notebook.\n",
    "\n",
    "# Sparse and iterative methods\n",
    "\n",
    "Many matrices in applications, particularly the study of physical systems and graphs/networks, have entries that are mostly equal to zero.  We can more efficiently store such systems by storing only the nonzero elements.  We will discuss storage and optimized implementations later.  Many of the methods for sparse systems apply to solving systems with matrices $A$ that can be applied to a vector ($y \\gets A x$) in significantly less than $O(n^2)$ complexity, or that are \"well-conditioned\" such that an iterative method converges in significantly less than $n$ iterations.\n",
    "\n",
    "[PETSc](https://mcs.anl.gov/petsc), the Portable Extensible Toolkit for Scientific computing, is an open source software package for the parallel solution of algebraic and differential-algebraic equations.  This includes linear algebra, for which PETSc offers a broad variety of implementations.  For general information about PETSc, I refer to [this primer](https://jedbrown.org/files/20150924-PETScPrimer.pdf).\n",
    "\n",
    "## Convergence of stationary iterative methods\n",
    "\n",
    "### Richardson iteration\n",
    "The simplest iterative method is [Richardson's method](https://en.wikipedia.org/wiki/Modified_Richardson_iteration), which solves $A x = b$ by the iteration\n",
    "$$ x_{k+1} = x_k + \\omega (b - A x_k) $$\n",
    "where $\\omega > 0$ is a damping parameter and $x_0$ is an initial guess (possibly the zero vector).  If $b = A x_*$, this iteration is equivalent to\n",
    "$$ x_{k+1} - x_* = (x_k - x_*) - \\omega A (x_k - x_*) = (I - \\omega A) (x_k - x_*) .$$\n",
    "It is convenient for convergence analysis to identify the \"error\" $e_k = x_k - x_*$, in which this becomes\n",
    "$$ e_{k+1} = (I - \\omega A) e_k $$\n",
    "or\n",
    "$$ e_k = (I - \\omega A)^k e_0 $$\n",
    "in terms of the initial error.  Evidently powers of the *iteration matrix* $I - \\omega A$ tell the whole story.\n",
    "Suppose that the eigendecomposition\n",
    "$$ X \\Lambda X^{-1} = I - \\omega A $$\n",
    "exists.  Then\n",
    "$$ (I - \\omega A)^k = (X \\Lambda X^{-1})^k = X \\Lambda^k X^{-1} $$\n",
    "and the convergence (or divergence) rate depends only on the largest magnitude eigenvalue.\n",
    "This analysis isn't great for two reasons:\n",
    "\n",
    "1. Not all matrices are diagonalizable.\n",
    "2. The matrix $X$ may be very ill-conditioned.\n",
    "\n",
    "We can repair these weaknesses by using the [Schur decomposition](https://en.wikipedia.org/wiki/Schur_decomposition)\n",
    "$$ Q R Q^h = I - \\omega A $$\n",
    "where $R$ is right-triangular and $Q$ is unitary (i.e., orthogonal if real-valued; $Q^h$ is the Hermitian conjugate of $Q$).\n",
    "The Schur decomposition always exists and $Q$ has a condition number of 1.\n",
    "\n",
    "* Where are the eigenvalues in $R$?\n",
    "\n",
    "Evidently we must find $\\omega$ to minimize the maximum eigenvalue of $I - \\omega A$.  We can do this if $A$ is well conditioned, but not in general.\n",
    "\n",
    "### Preconditioning\n",
    "\n",
    "Preconditioning is the act of creating an \"affordable\" operation \"$P^{-1}$\" such that $P^{-1} A$ (or $A P^{-1}$) is is well-conditoned or otherwise has a \"nice\" spectrum.  We then solve the system\n",
    "\n",
    "$$ P^{-1} A x = P^{-1} b \\quad \\text{or}\\quad A P^{-1} \\underbrace{(P x)}_y = b $$\n",
    "\n",
    "in which case the convergence rate depends on the spectrum of the iteration matrix\n",
    "$$ I - \\omega P^{-1} A . $$\n",
    "\n",
    "* The preconditioner must be applied on each iteration.\n",
    "* It is *not* merely about finding a good initial guess.\n",
    "\n",
    "There are two complementary techniques necessary for efficient iterative methods:\n",
    "\n",
    "* \"accelerators\" or Krylov methods, which use orthogonality to adaptively converge faster than Richardson\n",
    "* preconditioners that improve the spectrum of the preconditioned operator\n",
    "\n",
    "Although there is ongoing research in Krylov methods and they are immensely useful, I would say preconditioning is 90% of the game for practical applications, particularly as a research area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Krylov subspaces\n",
    "\n",
    "All matrix iterations work with approximations in a *Krylov subspace*, which has the form\n",
    "\n",
    "$$ K_n = \\big[ b \\big| Ab \\big| A^2 b \\big| \\dotsm \\big| A^{n-1} b \\big] . $$\n",
    "\n",
    "This matrix is horribly ill-conditioned and cannot stably be computed as written.  Instead, we seek an orthogonal basis $Q_n$ that spans the same space as $K_n$.  We could write this as a factorization\n",
    "\n",
    "$$ K_n = Q_n R_n $$\n",
    "\n",
    "where the first column $q_0 = b / \\lVert b \\rVert$.  The $R_n$ is unnecessary and hopelessly ill-conditioned, so a slightly different procedure is used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Arnoldi iteration\n",
    "\n",
    "The Arnoldi iteration applies orthogonal similarity transformations to reduce $A$ to [Hessenberg form](https://en.wikipedia.org/wiki/Hessenberg_matrix), starting from a vector $q_0 = b$,\n",
    "\n",
    "$$ A = Q H Q^h . $$\n",
    "\n",
    "Let's multiply on the right by $Q$ and examine the first $n$ columns,\n",
    "\n",
    "$$ A Q_n = Q_{n+1} H_n $$\n",
    "where $H_n$ is an $(n+1) \\times n$ Hessenberg matrix.\n",
    "\n",
    "#### Conditioning\n",
    "This representation is well-conditioned because $Q$ is orthogonal and\n",
    "\n",
    "$$ \\lVert H_n \\rVert \\le \\lVert Q_{n+1}^h \\rVert \\lVert A \\rVert \\lVert Q_n \\rVert \\le \\lVert A \\rVert $$.\n",
    "\n",
    "For a lower bound, we have\n",
    "\n",
    "$$ \\sigma_{\\min}(A)^2 \\rVert ^2 \\le x^h A^h A x $$\n",
    "\n",
    "for all $x$ of norm 1.  It must also be true for any $x = Q_n y$ where $\\lVert y\\rVert = 1$, thus\n",
    "\n",
    "$$ \\sigma_{\\min}(A)^2 \\le y^h Q_n^h A^h A Q_n y = y^h H_n^h Q_{n+1}^h Q_{n+1} H_n y = y^h H_n^h H_n y . $$\n",
    "\n",
    "#### GMRES\n",
    "\n",
    "GMRES (Generalized Minimum Residual) minimizes\n",
    "$$ \\lVert A x - b \\rVert $$\n",
    "over the subspace $Q_n$.  I.e., $x = Q_n y$ for some $y$.  By the recurrence above, this is equivalent to\n",
    "$$ \\lVert Q_{n+1} H_n y - b \\lVert $$\n",
    "which can be solved by minimizing\n",
    "$$ \\lVert H_n y - Q_{n+1}^h b \\rVert . $$\n",
    "This is achieved by incremental updates to a $QR$ factorization of $H_n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
